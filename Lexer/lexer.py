import ply.lex as lex
from Lexer.token import Token  # Assuming Token class is defined in token.py


class Lexer:
    def __init__(self, data):
        """
        Initializes the Lexer object with the given data.

        Parameters:
            data (str): The input data to be tokenized.
        """
        self.token = Token(data)  # Create a Token object with the input data
        self.lexer = lex.lex(object=self.token)  # Create the lexer object
        self.data = data

    def run(self):
        """
        Runs the lexer on the input data and displays token information in a table.
        """
        self.lexer.input(self.data)  # Set the input data for lexer
        toks = []
        for tok in self.lexer:  # Iterate through tokens generated by lexer
            toks.append(tok)
        self.show_in_table(toks)  # Display tokens in a table format

    def show_in_table(self, tokens):
        """
        Displays the token information in a table format.

        Parameters:
            tokens (list): List of tokens generated by the lexer.
        """
        # Define headers for the table
        headers = ["Line", "Column", "Token", "Value"]
        # Calculate the maximum lengths for each column
        max_lengths = [len(header) for header in headers]

        # Iterate through tokens to update max_lengths
        for tok in tokens:
            LINE, TOK, VALUE, COL = str(tok.lineno), str(tok.type), str(tok.value), str(tok.lexpos)
            max_lengths = [max(max_lengths[i], len(item)) for i, item in enumerate([LINE, COL, TOK, VALUE])]

        # Print the table headers
        header_line = "|"
        for header, length in zip(headers, max_lengths):
            header_line += f" {header.center(length)} |"
        print(header_line)
        print("-" * (sum(max_lengths) + 4 * len(headers)))  # Print a line separator

        # Iterate through tokens to print token information in the table
        for tok in tokens:
            # Calculate the starting position of the line containing the token
            line_start = self.data.rfind('\n', 0, tok.lexpos) + 1
            LINE, TOK, VALUE, COL = str(tok.lineno), str(tok.type), str(tok.value), str(tok.lexpos - line_start + 1)
            # Print token information formatted in the table
            print(
                f"| {LINE.strip().center(max_lengths[0])} | {COL.strip().center(max_lengths[1])} | {TOK.strip().center(max_lengths[2])} | {VALUE.strip().center(max_lengths[3])} |")
